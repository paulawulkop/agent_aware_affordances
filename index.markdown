---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---
<h1>{{ "Learning Agent-Aware Affordances for Closed-Loop Interaction with Articulated Objects."}}</h1>
<a href="https://github.com/jekyll">Giulio Schiavi*</a> &nbsp; 
<a href="https://scholar.google.com/citations?user=0k6BTKUAAAAJ&hl=de&oi=ao">Paula Wulkop*</a> &nbsp; 
<a href="https://github.com/jekyll">Giuseppe Rizzi</a> &nbsp; 
<a href="https://github.com/jekyll">Lionel Ott</a> &nbsp; 
<a href="https://github.com/jekyll">Roland Siegwart</a> &nbsp; 
<a href="https://github.com/jekyll">Jen Jen Chung<sup>1</sup></a>\
\* equal contribution

Authors are with the Autonomous Systems Lab, ETH Zurich, Switzerland.\
<sup>1</sup>Also with the School of ITEE, The University of Queensland, Australia.

You can find more information here:
<a href="https://github.com/jekyll">[Paper]</a> 
<a href="https://github.com/jekyll">[Code]</a> 


## Abstract
 Interactions with articulated objects are a challenging but important task for mobile robots. To tackle this challenge, we propose a novel closed-loop control pipeline, which integrates manipulation priors from affordance estimation with sampling-based whole-body control. We introduce the concept of agent-aware affordances which fully reflect the agent's capabilities and embodiment and we show that they outperform their state-of-the-art counterparts which are only conditioned on the end-effector geometry. Additionally, closed-loop affordance inference is found to allow the agent to divide a task into multiple non-continuous motions and recover from failure and unexpected states. Finally, the pipeline is able to perform long-horizon mobile manipulation tasks, i.e. opening and closing an oven, in the real world with high success rates (opening: 71%, closing: 72%).

## Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/lCS7L4SdVsc" title="YouTube video player" frameborder="0" allow="autoplay; picture-in-picture" allowfullscreen></iframe>

## Cite

## Acknowledgment
This project has received funding from the European Unionâ€™s Horizon 2020 research and innovation programme under grant agreement No 101017008 (Harmony).